{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from yelp_utils import *\n",
    "import yelp_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SEED_VAL = 200;\n",
    "# WORK_DIR = os.getcwd();\n",
    "# data_subset = \"_10Percent\"\n",
    "# YELP_DATA_CSV_DIR = os.path.join(WORK_DIR, \"data\", \"csv\")\n",
    "# YELP_DATA_WORD_2_VEC_MODEL_DIR = os.path.join(WORK_DIR, \"data\", \"word2vec_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Read the data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_utils.make_sure_path_exists(yelp_utils.YELP_DATA_WORD_2_VEC_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "read_filename = os.path.join(yelp_utils.YELP_DATA_CSV_DIR, 'business_review_user' + yelp_utils.data_subset + '.csv')\n",
    "df_data = pd.read_csv(read_filename, engine='c', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Great outdoor patio dining area. Great happy hour. Great service.\\r\\n\\r\\nOutdoor patio dining has a beautiful mesquite tree for ambiance and blocking out the sun while the center fireplace keeps you warm. \\r\\n\\r\\nWe had:\\r\\nQueso Skillet with warm tortilla chips - amazing!\\r\\nMac N Cheese with Chorizo - fabulous! one of the best mac n cheeses I've ever had!\\r\\nCarne Asada on a Potato Pancake - was ok. Sounded excellent, tasted decent.\\r\\n\\r\\nFriendly and good food. But the ambiance really puts it over the top as a great dining experience. I'd be back with a group of friends to lounge, play cornsack or bocce ball during happy hour.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.review_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords_and_stem(s):\n",
    "    '''\n",
    "    Function to convert a string to tokens and perform foloowing operations-\n",
    "    1. remove numbers \n",
    "    2. lowercase \n",
    "    3. remove stopwords as per NLTK stopword list\n",
    "    4. stem the words using Poerter stemmer\n",
    "    Input: String\n",
    "    Output: Token list\n",
    "    '''\n",
    "    s = yelp_utils.remove_numbers_in_string(s)\n",
    "    s = yelp_utils.lowercase_remove_punctuation(s)\n",
    "    s = yelp_utils.remove_stopwords(s)\n",
    "    token_list = nltk.word_tokenize(s)\n",
    "    #token_list = filter_out_more_stopwords(token_list)\n",
    "    token_list = yelp_utils.stem_token_list(token_list)\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'great', u'outdoor', u'patio', u'dine', u'area', u'great', u'happi', u'hour', u'great', u'servic', u'outdoor', u'patio', u'dine', u'beauti', u'mesquit', u'tree', u'ambianc', u'block', u'sun', u'center', u'fireplac', u'keep', u'warm', u'queso', u'skillet', u'warm', u'tortilla', u'chip', u'amaz', u'mac', u'n', u'chees', u'chorizo', u'fabul', u'one', u'best', u'mac', u'n', u'chees', u'ive', u'ever', u'carn', u'asada', u'potato', u'pancak', u'ok', u'sound', u'excel', u'tast', u'decent', u'friendli', u'good', u'food', u'ambianc', u'realli', u'put', u'top', u'great', u'dine', u'experi', u'id', u'back', u'group', u'friend', u'loung', u'play', u'cornsack', u'bocc', u'ball', u'happi', u'hour']\n"
     ]
    }
   ],
   "source": [
    "print lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords_and_stem(df_data.review_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    # source: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords_and_stem(raw_sentence))\n",
    "    \n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in df_data[\"review_text\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'locat', u'chevron', u'ga', u'station', u'lot']\n"
     ]
    }
   ],
   "source": [
    "print sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   Creating library C:/Users/hrushikesh/AppData/Local/Theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.12-64/cuda_ndarray/cuda_ndarray.lib and object C:/Users/hrushikesh/AppData/Local/Theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.12-64/cuda_ndarray/cuda_ndarray.exp\n",
      "\n",
      "Using gpu device 0: GeForce GTX 965M (CNMeM is disabled, CuDNN not available)\n",
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   Creating library C:/Users/hrushikesh/AppData/Local/Theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.12-64/tmpoegyrv/97496c4d3cf9a06dc4082cc141f918d2.lib and object C:/Users/hrushikesh/AppData/Local/Theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.12-64/tmpoegyrv/97496c4d3cf9a06dc4082cc141f918d2.exp\n",
      "\n",
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   Creating library C:/Users/hrushikesh/AppData/Local/Theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.12-64/tmpyanni6/6174b19f8005a60d6a2faaae7ff1c9a7.lib and object C:/Users/hrushikesh/AppData/Local/Theano/compiledir_Windows-10-10.0.10586-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.12-64/tmpyanni6/6174b19f8005a60d6a2faaae7ff1c9a7.exp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import gensim, logging\n",
    "from gensim.models import Word2Vec\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 8       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model_file = os.path.join(YELP_DATA_WORD_2_VEC_MODEL_DIR, str(num_features) + 'features_' + str(min_word_count) + 'minwords_' + str(context) + 'context'+ data_subset)\n",
    "if not os.path.isfile(model_file):\n",
    "    print \"Training model...\"\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                size=num_features, min_count = min_word_count, \\\n",
    "                window = context, sample = downsampling)\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "    model.save(model_file)\n",
    "else:\n",
    "    print \"Loading existing model\"\n",
    "    model = Word2Vec.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"amazing delightful bad\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "#     source: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "#     source: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000. == 0.:\n",
    "           print \"Review %d of %d\" % (counter, len(reviews))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 2193\n",
      "Review 1000 of 2193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:44: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 2000 of 2193\n",
      "Wall time: 423 ms\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in df_data[\"review_text\"]:\n",
    "    clean_train_reviews.append( lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords_and_stem( review))\n",
    "\n",
    "%time trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03380506, -0.03248994, -0.06748378, ..., -0.09760942,\n",
       "        -0.00110296,  0.0182657 ],\n",
       "       [ 0.02240651, -0.0220686 , -0.09725758, ..., -0.07256958,\n",
       "         0.01236789,  0.02137215],\n",
       "       [ 0.03171084, -0.03008272, -0.08868304, ..., -0.08954675,\n",
       "         0.01328464,  0.02270603],\n",
       "       ..., \n",
       "       [ 0.03102988, -0.03029333, -0.06572317, ..., -0.08929857,\n",
       "         0.01131339,  0.02390043],\n",
       "       [ 0.03002388, -0.02800074, -0.0602631 , ..., -0.08041041,\n",
       "         0.0128369 ,  0.02611707],\n",
       "       [ 0.02659599, -0.03020945, -0.07441082, ..., -0.07671879,\n",
       "         0.01343209,  0.02648316]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2193L, 300L)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec_feature_matrix_file = os.path.join(YELP_DATA_WORD_2_VEC_MODEL_DIR, 'word2vec_feature_matrix'+data_subset+'.csv')\n",
    "np.savetxt(word2vec_feature_matrix_file, trainDataVecs, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = np.genfromtxt(\n",
    "    word2vec_feature_matrix_file,           # file name\n",
    "    delimiter=',')           # column delimiter                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(trainDataVecs, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
